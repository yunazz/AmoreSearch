import os
import chromadb
import numpy as np
import aiomysql
import asyncio
import json
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.tools import tool
from langchain_core.messages import SystemMessage
from collections import defaultdict

from dotenv import load_dotenv
load_dotenv()

from pprint import pprint

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_MODEL = "gpt-4-turbo"

DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT"),
    "user": os.getenv("DB_USER"),  
    "password": os.getenv("DB_PASSWD"),
    "db": os.getenv("DB_NAME"),
    "autocommit": True,
}

client = chromadb.HttpClient(host="3.35.104.197", port=10090)

embedding_function = OpenAIEmbeddings(
    base_url="https://76mqtyy2wie64y-10050.proxy.runpod.net/v1",
    model="multilingual-e5-large",
    api_key="team3",
    tiktoken_enabled=False,
    embedding_ctx_length=502
)

ingredient_store = client.get_collection('ingredient')
cosmetic_store = client.get_collection('cosmetic')
brand_store = client.get_collection('brand')
post_store = client.get_collection('posts')

def get_fresh_llm():   # gpt-4-turbo ì¨ì•¼ í•¨! gpt-4 ì“°ë‹ˆê¹ multië¡œ toolì„ ì“°ì§€ ëª»í•¨!
    return ChatOpenAI(model_name=LLM_MODEL,  openai_api_key=OPENAI_API_KEY, temperature=0, cache=False)

@tool
def retrieve_ingredient(query: str, k: int = 5):
    """
    í™”ì¥í’ˆ ì„±ë¶„ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬. íŠ¹ì • ì„±ë¶„ì˜ íš¨ëŠ¥, ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€, í”¼ë¶€ íƒ€ì…ì— ë”°ë¥¸ ì¶”ì²œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    "ì˜ˆ: 'ë¯¸ë°± íš¨ê³¼ê°€ ìˆëŠ” ì„±ë¶„ì€?', 'ì§€ì„± í”¼ë¶€ì— ì¢‹ì€ ì„±ë¶„ ì¶”ì²œí•´ì¤˜', 'ìì™¸ì„  ì°¨ë‹¨ ì„±ë¶„ ì•Œë ¤ì¤˜'
    """
    try:
        query_embedding = embedding_function.embed_query(query)
        
        results = ingredient_store.query(query_embeddings=query_embedding, n_results=k)
        return results if results['documents'] else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"Error retrieving ingredients: {str(e)}"

@tool
def retrieve_cosmetic(query: str, k: int = 3):
    """
    í™”ì¥í’ˆ ì œí’ˆ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬. íŠ¹ì • ê¸°ëŠ¥, ë¸Œëœë“œ, í”¼ë¶€ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í™”ì¥í’ˆì„ ì¶”ì²œí•©ë‹ˆë‹¤. 
    ì˜ˆ: 'ë¯¸ë°± íš¨ê³¼ê°€ ìˆëŠ” í™”ì¥í’ˆ ì¶”ì²œí•´ì¤˜', 'ì—¬ë“œë¦„ í”¼ë¶€ì— ì¢‹ì€ ì œí’ˆ 3ê°œ ì•Œë ¤ì¤˜', 'ìì‚¬ ë¸Œëœë“œ ì œí’ˆ ì†Œê°œí•´ì¤˜'
    """
    try:
        query_embedding = embedding_function.embed_query(query)
        
        results = cosmetic_store.query(query_embeddings=query_embedding, n_results=k)
        return results if results['documents'] else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"Error retrieving products: {str(e)}"

@tool
def retrieve_brand(query: str, k: int = 5):
    """
    í™”ì¥í’ˆ ë¸Œëœë“œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬. ë¸Œëœë“œì˜ íŠ¹ì§•, ì£¼ìš” ì œí’ˆ, ì² í•™(ì˜ˆ: ë¹„ê±´, ìœ ê¸°ë† ë“±)ì„ ì œê³µí•©ë‹ˆë‹¤.
    ì˜ˆ: 'ì´ë‹ˆìŠ¤í”„ë¦¬ ë¸Œëœë“œ ìŠ¤í† ë¦¬ ì•Œë ¤ì¤˜', 'ë¹„ê±´ í™”ì¥í’ˆ ë¸Œëœë“œ ì¶”ì²œí•´ì¤˜', 'í—¤ì–´ ê´€ë ¨ ìì‚¬ ë¸Œëœë“œ ì•Œë ¤ì¤˜'
    """
    try:
        query_embedding = embedding_function.embed_query(query)
        
        results = brand_store.query(query_embeddings=query_embedding, n_results=k)
        return results if results['documents'] else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"Error retrieving brands: {str(e)}"

@tool
def retrieve_posts(query: str, k: int = 5):
    """
    ìµœê·¼ í™”ì¥í’ˆ ê´€ë ¨ ë‰´ìŠ¤, íŠ¸ë Œë“œ, ì—°êµ¬ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬. ë·°í‹° ì—…ê³„ ìµœì‹  ì†Œì‹ì„ ì œê³µí•©ë‹ˆë‹¤. 
    ì˜ˆ: 'ìµœê·¼ íŠ¸ë Œë“œì¸ í™”ì¥í’ˆ ì„±ë¶„ì´ ë­ì•¼?', 'í™”ì¥í’ˆ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ 2ê°œ ì°¾ì•„ì¤˜', 'ì¹œí™˜ê²½ í™”ì¥í’ˆ íŠ¸ë Œë“œ ê¸°ì‚¬ ì•Œë ¤ì¤˜'
    """
    try:
        query_embedding = embedding_function.embed_query(query)
        
        results = post_store.query(query_embeddings=query_embedding, n_results=k)
        return results if results['documents'] else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"Error retrieving posts: {str(e)}"

tool_list = [retrieve_ingredient, retrieve_cosmetic, retrieve_brand, retrieve_posts] 

def get_retrieved_documnets(query: str):
    """LLMê³¼ ë„êµ¬ í˜¸ì¶œì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""

    messages = [    # ì˜ì–´ë¡œ ì“°ë‹ˆê¹ ë” ì˜ ì´í•´í•˜ëŠ” ê²½í–¥ì„ ë³´ì„!
        SystemMessage(content="Analyze the user's query and call **all relevant tools** simultaneously. "
                        "For example, if the user asks for cosmetic, ingredients and research papers, "
                        "you must invoke all the tools at once."),
        HumanMessage(query)
    ]
    
    llm = get_fresh_llm()
    llm_with_tools = llm.bind_tools(tools=tool_list, tool_choice="required")
    
    ai_msg = llm_with_tools.invoke(messages)

    tool_outputs = []
    tool_dict = {
        "retrieve_ingredient": retrieve_ingredient,
        "retrieve_cosmetic": retrieve_cosmetic,
        "retrieve_brand": retrieve_brand,
        "retrieve_posts": retrieve_posts
    }

    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
        for tool_call in ai_msg.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]

            if tool_name in tool_dict:
                selected_tool = tool_dict[tool_name]
                tool_output = selected_tool.invoke(input=tool_args)  
                
                tool_outputs.append({"tool_name": tool_name, "output": tool_output})


    return tool_outputs

def generate_context(doc_datas, max_tokens=500):
    """
    ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ì—¬ LLMì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
    - doc_datas (list of dict): (ê° í•­ëª©ì— 'distance'ì™€ 'document' í¬í•¨)
    - max_tokens (int): ìµœëŒ€ í—ˆìš© í† í° ìˆ˜ (ê¸°ë³¸ê°’ 500)

    Returns:
    - final_context (str): ì •ì œëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    # 1. ê±°ë¦¬ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì‘ì„ìˆ˜ë¡ ì¤‘ìš”!)
    sorted_data = sorted(doc_datas, key=lambda x: x['distance'])
    
    # 2. ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (ê° ë¬¸ì„œë¥¼ \nìœ¼ë¡œ ì—°ê²°)
    all_texts = [item['document'] for item in sorted_data]
    full_context = "\n\n".join(all_texts)

    # 3. í† í° ì œí•œ ë‚´ì—ì„œ ë¬¸ì„œ ìë¥´ê¸°
    words = full_context.split() 
    if len(words) > max_tokens:
        trimmed_context = " ".join(words[:max_tokens])  # max_tokens ë§Œí¼ë§Œ ìœ ì§€í•˜ê³ 
    else:
        trimmed_context = full_context # ì œí•œë³´ë‹¤ ì ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    return trimmed_context

def create_prompt(context, question):
    """
    LLMì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    
    Parameters:
    - context (str): ì •ì œëœ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
    - question (str): ì‚¬ìš©ìê°€ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸
    
    Returns:
    - prompt (str): LLMì„ í˜¸ì¶œí•  ìµœì ì˜ í”„ë¡¬í”„íŠ¸
    """
    prompt_template = f"""ë„ˆëŠ” ì•„ëª¨ë ˆí¼ì‹œí”½ íšŒì‚¬ ì „ë¬¸ ë¹„ì„œì•¼. ì•„ë˜ ì œê³µëœ ì„±ë¶„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì¤˜.
    
    --- [ì„±ë¶„ ì •ë³´] ---
    {context}
    --------------------
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸:
    {question}
    
    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ê°€ì²˜ëŸ¼ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì¤˜.
    """
    
    return prompt_template

async def query_mariadb(query, params=None):
    """
    ë¹„ë™ê¸°ì ìœ¼ë¡œ MariaDB ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    - query: ì‹¤í–‰í•  SQL ì¿¼ë¦¬ (ë¬¸ìì—´)
    - params: SQLì— ë°”ì¸ë”©í•  íŒŒë¼ë¯¸í„° (íŠœí”Œ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
    
    Returns:
    - ì¡°íšŒëœ ê²°ê³¼ (list)
    """
    conn = None
    result = []
    
    try:
        conn = await aiomysql.connect(**DB_CONFIG)
        async with conn.cursor(aiomysql.DictCursor) as cur:
            await cur.execute(query, params)
            result = await cur.fetchall()

        conn.close()
        return result

    except aiomysql.Error as e:
        print(f"[MariaDB ì—ëŸ¬]: {e}")
        return None


async def generate_llm_response(query, retrieved_collections_docs):
    """
    LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸°)
    """
    flatten_docs = []
    for collection in retrieved_collections_docs:
        for index in range(len(collection.get('output').get('distances')[0])):
            doc = {
                'distance': collection.get('output').get('distances')[0][index],
                'document': collection.get('output').get('documents')[0][index]
            }
            flatten_docs.append(doc)

    context = generate_context(flatten_docs, 7000)

    prompt = create_prompt(context, query)

    llm_init = get_fresh_llm()
    response = llm_init.invoke(prompt)
    
    print("[LLM ì‘ë‹µ]:")
    pprint(response.content)
    
    return response.content


async def fetch_mariadb_data(retrieved_collections_docs):
    ingredient_ids = []
    cosmetic_queries = defaultdict(list)  # scopeë³„ë¡œ ë¬¶ê¸°
    post_queries = defaultdict(list)  # scopeë³„ë¡œ ë¬¶ê¸°

    for collection in retrieved_collections_docs:
        tool_name = collection.get('tool_name')
        metadatas = collection.get('output').get('metadatas')[0]

        if tool_name == "retrieve_ingredient":
            for meta in metadatas:
                ingredient_ids.append(meta.get('ingred_id'))
        
        elif tool_name == "retrieve_cosmetic":
            for meta in metadatas:
                scope = meta.get('scope')
                cosmetic_queries[scope].append(meta.get('cosmetic_id'))
        
        elif tool_name == "retrieve_posts":
            for meta in metadatas:
                scope = meta.get('scope')
                post_queries[scope].append(meta.get('post_id'))
                
    ingredient_results = None
    if ingredient_ids:
        ingredient_placeholders = ",".join(["%s"] * len(ingredient_ids))
        ingredient_results = await query_mariadb(
            f"SELECT * FROM ingredient WHERE id IN ({ingredient_placeholders})",
            tuple(ingredient_ids) 
        )

    cosmetic_results = {}
    for scope, cosmetic_ids in cosmetic_queries.items():
        if not cosmetic_ids:
            continue

        table_name = "cosmetic" if scope == 'ìì‚¬' else "cosmetic_external"
        cosmetic_placeholders = ",".join(["%s"] * len(cosmetic_ids))
        cosmetic_results[scope] = await query_mariadb(
            f"SELECT * FROM {table_name} WHERE cosmetic_id IN ({cosmetic_placeholders})",
            tuple(cosmetic_ids) 
        )

    post_results = {}
    for scope, post_ids in post_queries.items():
        if not post_ids:
            continue

        table_name = "post" if scope == 'INTERNAL' else "cosmetic_external"
        post_placeholders = ",".join(["%s"] * len(post_ids)) 
        post_results[scope] = await query_mariadb(
            f"SELECT * FROM {table_name} WHERE post_id IN ({post_placeholders})",
            tuple(post_ids) 
        )

    print("ğŸ“Œ [MariaDB ì¡°íšŒ ê²°ê³¼]:")
    pprint({
        "ingredient": ingredient_results,
        "cosmetic": cosmetic_results,
        "post": post_results
    })

    return {
        "ingredient": ingredient_results,
        "cosmetic": cosmetic_results,
        "post": post_results
    }
    
async def get_ai_search_response(query):
    retrieved_collections_docs = get_retrieved_documnets(query)

    llm_task = asyncio.create_task(generate_llm_response(query, retrieved_collections_docs))
    db_task = asyncio.create_task(fetch_mariadb_data(retrieved_collections_docs))

    llm_response, db_response = await asyncio.gather(llm_task, db_task)

    return {"llm_response": llm_response, "db_response": db_response}


class IntegrationSearch:
    @staticmethod
    async def search(question: str):
        """AI ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œ (ë¹„ë™ê¸°)"""
        try:
            return get_ai_search_response(question)
        except Exception as e:
            print(e)
            
class AISearch:
    @staticmethod
    async def search(question: str):
        llm = ChatOpenAI(model=LLM_MODEL, openai_api_key=OPENAI_API_KEY, model_kwargs={"stream": True})
        
        async for chunk in llm.astream(question):
            if chunk.content:
                yield chunk.content  
            await asyncio.sleep(0.1)
    
